# Быстрый старт

## Вариант 1: С Perplexity API (рекомендуется для начала)

### 1. Получите API ключ

Зарегистрируйтесь на https://www.perplexity.ai/settings/api и получите ключ.

### 2. Настройте конфигурацию

Отредактируйте `config/api_keys.yaml`:

```yaml
llm_provider: "perplexity"

perplexity:
  api_key: "ваш-ключ-здесь"
```

### 3. Запустите приложение

```bash
source venv/bin/activate  # или venv\Scripts\activate на Windows
python src/main.py
```

### 4. Попробуйте

```
> Привет! Как дела?
Assistant: Привет! У меня всё отлично...

> /help
# Увидите список доступных команд
```

---

## Вариант 2: С локальной моделью (бесплатно, но медленнее)

### 1. Установите Ollama

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Скачайте с https://ollama.ai/download

### 2. Загрузите модель

```bash
ollama run qwen3:8b
# Дождитесь загрузки модели (около 4.7 GB)
# После загрузки нажмите Ctrl+D для выхода
```

### 3. Настройте конфигурацию

Отредактируйте `config/api_keys.yaml`:

```yaml
llm_provider: "local"

perplexity:
  api_key: "не-требуется"
```

Проверьте `config/local_llm_config.yaml`:

```yaml
chat_model:
  host: "localhost"
  port: 11434
  model_name: "qwen3:8b"
  temperature: 0.7
```

### 4. Запустите приложение

```bash
source venv/bin/activate
python src/main.py
```

При запуске вы должны увидеть:
```
[LLM] Используется локальная модель: qwen3:8b на localhost:11434
```

---

## Индексация документации (опционально)

Для использования RAG системы:

1. Поместите документы в папку `docs/` (форматы: `.txt`, `.md`)

2. Запустите индексацию:
```
> /index
Начинаю индексацию документов...
Индексация завершена!
Файлов: 5
Чанков: 42
Ошибок: 0
```

3. Теперь ассистент будет использовать эти документы при ответах:
```
> Как создать тикет?
Assistant: [ищет в документации и отвечает на основе найденного]
```

---

## Устранение неполадок

### Perplexity: "Неверный API ключ"
- Проверьте, что ключ скопирован полностью
- Убедитесь, что на аккаунте есть кредиты

### Локальная модель: "Модель недоступна"
```bash
# Проверьте, что Ollama запущен
ollama list

# Если модели нет в списке, загрузите её
ollama run qwen3:8b

# Проверьте, что порт 11434 свободен
lsof -i :11434  # macOS/Linux
netstat -ano | findstr :11434  # Windows
```

### Медленная генерация (локальная модель)
- Это нормально для первого запроса (модель загружается в память)
- Последующие запросы будут быстрее
- Попробуйте более легкую модель: `ollama run phi3`

---

## Переключение между провайдерами

Просто измените `llm_provider` в `config/api_keys.yaml`:

```yaml
# Для Perplexity
llm_provider: "perplexity"

# Для локальной модели
llm_provider: "local"
```

Перезапустите приложение.

---

## Что дальше?

- Прочитайте [README.md](README.md) для полного понимания архитектуры
- Изучите [docs/local_llm_setup.md](docs/local_llm_setup.md) для продвинутой настройки
- Попробуйте разные модели: `llama3`, `mistral`, `phi3`
- Настройте MCP серверы для работы с тикетами
