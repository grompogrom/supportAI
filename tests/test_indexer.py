"""
–¢–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
"""

import unittest
from unittest.mock import Mock, patch, MagicMock
import tempfile
import shutil
import os
import json
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from rag.indexer import (
    DocumentIndexer,
    DocumentChunk,
    IndexingResult,
    IndexerError,
)


class TestDocumentChunk(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –¥–ª—è DocumentChunk dataclass."""
    
    def test_chunk_creation(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–∞."""
        chunk = DocumentChunk(
            chunk_id="test_chunk_0001",
            text="Sample text content",
            source_file="docs/test.md",
            position=0
        )
        self.assertEqual(chunk.chunk_id, "test_chunk_0001")
        self.assertEqual(chunk.text, "Sample text content")
        self.assertEqual(chunk.source_file, "docs/test.md")
        self.assertEqual(chunk.position, 0)


class TestIndexingResult(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –¥–ª—è IndexingResult dataclass."""
    
    def test_result_creation(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
        result = IndexingResult(
            total_files=5,
            total_chunks=20,
            indexed_files=["file1.md", "file2.md"],
            errors=["file3.md: error"]
        )
        self.assertEqual(result.total_files, 5)
        self.assertEqual(result.total_chunks, 20)
        self.assertEqual(len(result.indexed_files), 2)
        self.assertEqual(len(result.errors), 1)


class TestDocumentIndexerInit(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DocumentIndexer."""
    
    def setUp(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤."""
        self.temp_dir = tempfile.mkdtemp()
        self.docs_dir = os.path.join(self.temp_dir, "docs")
        self.embeddings_path = os.path.join(self.temp_dir, "data", "embeddings.json")
    
    def tearDown(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        shutil.rmtree(self.temp_dir)
    
    def test_init_creates_docs_dir(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ docs –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏."""
        self.assertFalse(os.path.exists(self.docs_dir))
        
        indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
        
        self.assertTrue(os.path.exists(self.docs_dir))
    
    def test_init_stores_parameters(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."""
        os.makedirs(self.docs_dir)
        
        indexer = DocumentIndexer(
            self.docs_dir,
            self.embeddings_path,
            chunk_size=100,
            overlap=20
        )
        
        self.assertEqual(indexer._docs_dir, self.docs_dir)
        self.assertEqual(indexer._embeddings_path, self.embeddings_path)
        self.assertEqual(indexer._chunk_size, 100)
        self.assertEqual(indexer._overlap, 20)
    
    def test_init_default_values(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        os.makedirs(self.docs_dir)
        
        indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
        
        self.assertEqual(indexer._chunk_size, 500)
        self.assertEqual(indexer._overlap, 50)


class TestScanDocuments(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –¥–ª—è scan_documents."""
    
    def setUp(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏."""
        self.temp_dir = tempfile.mkdtemp()
        self.docs_dir = os.path.join(self.temp_dir, "docs")
        os.makedirs(self.docs_dir)
        self.embeddings_path = os.path.join(self.temp_dir, "embeddings.json")
        
        # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
        self._create_file("test1.md", "# Test 1")
        self._create_file("test2.txt", "Test 2 content")
        self._create_file("test3.MD", "# Test 3 uppercase ext")
        self._create_file("ignored.py", "# Not a doc")
        self._create_file("ignored.json", "{}")
        
        # –°–æ–∑–¥–∞—ë–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å —Ñ–∞–π–ª–∞–º–∏
        subdir = os.path.join(self.docs_dir, "subdir")
        os.makedirs(subdir)
        self._create_file("subdir/nested.md", "# Nested doc")
    
    def tearDown(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        shutil.rmtree(self.temp_dir)
    
    def _create_file(self, relative_path, content):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –≤ docs –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        full_path = os.path.join(self.docs_dir, relative_path)
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def test_scan_finds_md_files(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∏—Å–∫–∞ .md —Ñ–∞–π–ª–æ–≤."""
        indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
        files = indexer.scan_documents()
        
        md_files = [f for f in files if f.endswith('.md') or f.endswith('.MD')]
        self.assertEqual(len(md_files), 3)  # test1.md, test3.MD, nested.md
    
    def test_scan_finds_txt_files(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∏—Å–∫–∞ .txt —Ñ–∞–π–ª–æ–≤."""
        indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
        files = indexer.scan_documents()
        
        txt_files = [f for f in files if f.endswith('.txt')]
        self.assertEqual(len(txt_files), 1)
    
    def test_scan_ignores_unsupported_extensions(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π."""
        indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
        files = indexer.scan_documents()
        
        py_files = [f for f in files if f.endswith('.py')]
        json_files = [f for f in files if f.endswith('.json')]
        
        self.assertEqual(len(py_files), 0)
        self.assertEqual(len(json_files), 0)
    
    def test_scan_recursive(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."""
        indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
        files = indexer.scan_documents()
        
        nested_files = [f for f in files if 'subdir' in f]
        self.assertEqual(len(nested_files), 1)
    
    def test_scan_returns_sorted_list(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
        files = indexer.scan_documents()
        
        self.assertEqual(files, sorted(files))
    
    def test_scan_empty_directory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Å—Ç–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        empty_dir = os.path.join(self.temp_dir, "empty_docs")
        os.makedirs(empty_dir)
        
        indexer = DocumentIndexer(empty_dir, self.embeddings_path)
        files = indexer.scan_documents()
        
        self.assertEqual(files, [])


class TestReadDocument(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –¥–ª—è read_document."""
    
    def setUp(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        self.temp_dir = tempfile.mkdtemp()
        self.docs_dir = os.path.join(self.temp_dir, "docs")
        os.makedirs(self.docs_dir)
        self.embeddings_path = os.path.join(self.temp_dir, "embeddings.json")
        self.indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
    
    def tearDown(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        shutil.rmtree(self.temp_dir)
    
    def test_read_utf8_file(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–µ–Ω–∏—è UTF-8 —Ñ–∞–π–ª–∞."""
        file_path = os.path.join(self.docs_dir, "test.md")
        content = "# –ó–∞–≥–æ–ª–æ–≤–æ–∫\n\n–¢–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        result = self.indexer.read_document(file_path)
        
        self.assertEqual(result, content)
    
    def test_read_strips_whitespace(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤."""
        file_path = os.path.join(self.docs_dir, "test.md")
        content = "  \n\nContent\n\n  "
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        result = self.indexer.read_document(file_path)
        
        self.assertEqual(result, "Content")
    
    def test_read_latin1_fallback(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ fallback –Ω–∞ latin-1 –∫–æ–¥–∏—Ä–æ–≤–∫—É."""
        file_path = os.path.join(self.docs_dir, "test.txt")
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤ latin-1
        with open(file_path, 'wb') as f:
            f.write(b"Caf\xe9 content")  # latin-1 encoded "Caf√©"
        
        result = self.indexer.read_document(file_path)
        
        self.assertIn("Caf", result)
    
    def test_read_nonexistent_file(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—à–∏–±–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞."""
        file_path = os.path.join(self.docs_dir, "nonexistent.md")
        
        with self.assertRaises(FileNotFoundError):
            self.indexer.read_document(file_path)


class TestSplitIntoChunks(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –¥–ª—è split_into_chunks."""
    
    def setUp(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        self.temp_dir = tempfile.mkdtemp()
        self.docs_dir = os.path.join(self.temp_dir, "docs")
        os.makedirs(self.docs_dir)
        self.embeddings_path = os.path.join(self.temp_dir, "embeddings.json")
    
    def tearDown(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        shutil.rmtree(self.temp_dir)
    
    def test_chunking_with_overlap_spec_example(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —á–∞–Ω–∫–∏–Ω–≥–∞ –∏–∑ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=10, overlap=3
        )
        
        text = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
        chunks = list(indexer.split_into_chunks(text, "test.txt"))
        
        self.assertEqual(len(chunks), 4)
        
        # Chunk 1: ABCDEFGHIJ (position 0)
        self.assertEqual(chunks[0].text, "ABCDEFGHIJ")
        self.assertEqual(chunks[0].position, 0)
        
        # Chunk 2: HIJKLMNOPQ (position 7)
        self.assertEqual(chunks[1].text, "HIJKLMNOPQ")
        self.assertEqual(chunks[1].position, 7)
        
        # Chunk 3: OPQRSTUVWX (position 14)
        self.assertEqual(chunks[2].text, "OPQRSTUVWX")
        self.assertEqual(chunks[2].position, 14)
        
        # Chunk 4: VWXYZ (position 21, shorter)
        self.assertEqual(chunks[3].text, "VWXYZ")
        self.assertEqual(chunks[3].position, 21)
    
    def test_chunk_ids_are_unique(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏ chunk_id."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=10, overlap=3
        )
        
        text = "A" * 100
        chunks = list(indexer.split_into_chunks(text, "test.txt"))
        
        chunk_ids = [c.chunk_id for c in chunks]
        self.assertEqual(len(chunk_ids), len(set(chunk_ids)))
    
    def test_chunk_id_format(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ chunk_id."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=10, overlap=3
        )
        
        chunks = list(indexer.split_into_chunks("ABCDEFGHIJ", "example.md"))
        
        self.assertEqual(chunks[0].chunk_id, "example_chunk_0000")
    
    def test_single_chunk_for_short_text(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=100, overlap=10
        )
        
        text = "Short text"
        chunks = list(indexer.split_into_chunks(text, "test.txt"))
        
        self.assertEqual(len(chunks), 1)
        self.assertEqual(chunks[0].text, "Short text")
    
    def test_empty_text_no_chunks(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=10, overlap=3
        )
        
        chunks = list(indexer.split_into_chunks("", "test.txt"))
        
        self.assertEqual(len(chunks), 0)
    
    def test_source_file_in_chunk(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è source_file –≤ —á–∞–Ω–∫–µ."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=10, overlap=3
        )
        
        source = "docs/subdir/myfile.md"
        chunks = list(indexer.split_into_chunks("ABCDEFGHIJ", source))
        
        self.assertEqual(chunks[0].source_file, source)


class TestGenerateChunkId(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –¥–ª—è _generate_chunk_id."""
    
    def setUp(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        self.temp_dir = tempfile.mkdtemp()
        self.docs_dir = os.path.join(self.temp_dir, "docs")
        os.makedirs(self.docs_dir)
        self.embeddings_path = os.path.join(self.temp_dir, "embeddings.json")
        self.indexer = DocumentIndexer(self.docs_dir, self.embeddings_path)
    
    def tearDown(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        shutil.rmtree(self.temp_dir)
    
    def test_generate_chunk_id_format(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ ID."""
        chunk_id = self.indexer._generate_chunk_id("docs/example.md", 0)
        self.assertEqual(chunk_id, "example_chunk_0000")
    
    def test_generate_chunk_id_with_position(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ ID —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–æ–∑–∏—Ü–∏—è–º–∏."""
        self.assertEqual(
            self.indexer._generate_chunk_id("test.txt", 0),
            "test_chunk_0000"
        )
        self.assertEqual(
            self.indexer._generate_chunk_id("test.txt", 1),
            "test_chunk_0001"
        )
        self.assertEqual(
            self.indexer._generate_chunk_id("test.txt", 99),
            "test_chunk_0099"
        )
        self.assertEqual(
            self.indexer._generate_chunk_id("test.txt", 9999),
            "test_chunk_9999"
        )
    
    def test_generate_chunk_id_with_path(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ ID —Å –ø–æ–ª–Ω—ã–º –ø—É—Ç—ë–º –∫ —Ñ–∞–π–ª—É."""
        chunk_id = self.indexer._generate_chunk_id(
            "/home/user/docs/subdir/myfile.md", 5
        )
        self.assertEqual(chunk_id, "myfile_chunk_0005")


class TestSaveAndLoadIndex(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –¥–ª—è save_index –∏ load_index."""
    
    def setUp(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        self.temp_dir = tempfile.mkdtemp()
        self.docs_dir = os.path.join(self.temp_dir, "docs")
        os.makedirs(self.docs_dir)
        self.embeddings_path = os.path.join(self.temp_dir, "data", "embeddings.json")
        self.indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=100, overlap=10
        )
    
    def tearDown(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        shutil.rmtree(self.temp_dir)
    
    def test_save_creates_directory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏."""
        chunks = [
            DocumentChunk("chunk_0000", "Text 1", "file1.md", 0),
        ]
        embeddings = [[0.1, 0.2, 0.3]]
        
        self.assertFalse(os.path.exists(os.path.dirname(self.embeddings_path)))
        
        self.indexer.save_index(chunks, embeddings)
        
        self.assertTrue(os.path.exists(self.embeddings_path))
    
    def test_save_and_load_roundtrip(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞."""
        chunks = [
            DocumentChunk("chunk_0000", "Text 1", "file1.md", 0),
            DocumentChunk("chunk_0001", "Text 2", "file2.md", 100),
        ]
        embeddings = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]
        
        self.indexer.save_index(chunks, embeddings)
        loaded = self.indexer.load_index()
        
        self.assertIsNotNone(loaded)
        self.assertIn("indexed_at", loaded)
        self.assertIn("config", loaded)
        self.assertIn("chunks", loaded)
        
        self.assertEqual(loaded["config"]["chunk_size"], 100)
        self.assertEqual(loaded["config"]["overlap"], 10)
        
        self.assertEqual(len(loaded["chunks"]), 2)
        self.assertEqual(loaded["chunks"][0]["id"], "chunk_0000")
        self.assertEqual(loaded["chunks"][0]["text"], "Text 1")
        self.assertEqual(loaded["chunks"][0]["embedding"], [0.1, 0.2, 0.3])
        self.assertEqual(loaded["chunks"][0]["source"], "file1.md")
        self.assertEqual(loaded["chunks"][0]["position"], 0)
    
    def test_load_nonexistent_returns_none(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞."""
        result = self.indexer.load_index()
        self.assertIsNone(result)
    
    def test_is_index_exists_false(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ is_index_exists –¥–ª—è –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞."""
        self.assertFalse(self.indexer.is_index_exists())
    
    def test_is_index_exists_true(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ is_index_exists –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ–∞–π–ª–∞."""
        chunks = [DocumentChunk("chunk_0000", "Text", "file.md", 0)]
        embeddings = [[0.1, 0.2]]
        
        self.indexer.save_index(chunks, embeddings)
        
        self.assertTrue(self.indexer.is_index_exists())
    
    def test_save_unicode_content(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Unicode –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
        chunks = [
            DocumentChunk("chunk_0000", "–¢–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º üéâ", "file.md", 0),
        ]
        embeddings = [[0.1, 0.2]]
        
        self.indexer.save_index(chunks, embeddings)
        loaded = self.indexer.load_index()
        
        self.assertEqual(loaded["chunks"][0]["text"], "–¢–µ–∫—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º üéâ")


class TestIndexAll(unittest.TestCase):
    """–¢–µ—Å—Ç—ã –¥–ª—è index_all."""
    
    def setUp(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏."""
        self.temp_dir = tempfile.mkdtemp()
        self.docs_dir = os.path.join(self.temp_dir, "docs")
        os.makedirs(self.docs_dir)
        self.embeddings_path = os.path.join(self.temp_dir, "embeddings.json")
        
        # –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        with open(os.path.join(self.docs_dir, "doc1.md"), 'w') as f:
            f.write("First document content")
        with open(os.path.join(self.docs_dir, "doc2.txt"), 'w') as f:
            f.write("Second document content")
    
    def tearDown(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        shutil.rmtree(self.temp_dir)
    
    def test_index_all_calls_embedding_generator(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã–∑–æ–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=50, overlap=5
        )
        
        # –ú–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —ç–º–±–µ–¥–∏–Ω–≥–æ–≤
        mock_generator = Mock()
        mock_generator.generate.return_value = [0.1, 0.2, 0.3]
        
        result = indexer.index_all(mock_generator)
        
        # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        self.assertGreater(mock_generator.generate.call_count, 0)
    
    def test_index_all_returns_correct_result(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑–≤—Ä–∞—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=50, overlap=5
        )
        
        mock_generator = Mock()
        mock_generator.generate.return_value = [0.1, 0.2, 0.3]
        
        result = indexer.index_all(mock_generator)
        
        self.assertIsInstance(result, IndexingResult)
        self.assertEqual(result.total_files, 2)
        self.assertGreater(result.total_chunks, 0)
        self.assertEqual(len(result.indexed_files), 2)
        self.assertEqual(len(result.errors), 0)
    
    def test_index_all_saves_index(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=50, overlap=5
        )
        
        mock_generator = Mock()
        mock_generator.generate.return_value = [0.1, 0.2, 0.3]
        
        indexer.index_all(mock_generator)
        
        self.assertTrue(os.path.exists(self.embeddings_path))
        
        with open(self.embeddings_path, 'r') as f:
            data = json.load(f)
        
        self.assertIn("chunks", data)
        self.assertGreater(len(data["chunks"]), 0)
    
    def test_index_all_handles_errors(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫."""
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=50, overlap=5
        )
        
        # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø–∞–¥–∞–µ—Ç –Ω–∞ –≤—Ç–æ—Ä–æ–º —Ñ–∞–π–ª–µ
        mock_generator = Mock()
        call_count = [0]
        
        def side_effect(text):
            call_count[0] += 1
            if "Second" in text:
                raise Exception("Test error")
            return [0.1, 0.2, 0.3]
        
        mock_generator.generate.side_effect = side_effect
        
        result = indexer.index_all(mock_generator)
        
        # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–¥–Ω–∞ –æ—à–∏–±–∫–∞
        self.assertEqual(len(result.errors), 1)
        self.assertIn("doc2.txt", result.errors[0])
    
    def test_index_all_empty_directory(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –ø—É—Å—Ç–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        empty_dir = os.path.join(self.temp_dir, "empty_docs")
        os.makedirs(empty_dir)
        empty_embeddings = os.path.join(self.temp_dir, "empty_embeddings.json")
        
        indexer = DocumentIndexer(empty_dir, empty_embeddings)
        mock_generator = Mock()
        
        result = indexer.index_all(mock_generator)
        
        self.assertEqual(result.total_files, 0)
        self.assertEqual(result.total_chunks, 0)
        mock_generator.generate.assert_not_called()


class TestIntegration(unittest.TestCase):
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã."""
    
    def setUp(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        self.temp_dir = tempfile.mkdtemp()
        self.docs_dir = os.path.join(self.temp_dir, "docs")
        os.makedirs(self.docs_dir)
        self.embeddings_path = os.path.join(self.temp_dir, "embeddings.json")
    
    def tearDown(self):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏."""
        shutil.rmtree(self.temp_dir)
    
    def test_full_indexing_workflow(self):
        """–ü–æ–ª–Ω—ã–π workflow –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
        # –°–æ–∑–¥–∞—ë–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        doc1_content = "# –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n\n" + "–ê" * 600
        doc2_content = "# FAQ\n\n" + "–ë" * 400
        
        with open(os.path.join(self.docs_dir, "guide.md"), 'w', encoding='utf-8') as f:
            f.write(doc1_content)
        with open(os.path.join(self.docs_dir, "faq.md"), 'w', encoding='utf-8') as f:
            f.write(doc2_content)
        
        # –°–æ–∑–¥–∞—ë–º –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä
        indexer = DocumentIndexer(
            self.docs_dir, self.embeddings_path,
            chunk_size=200, overlap=20
        )
        
        # –ú–æ–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        mock_generator = Mock()
        embedding_counter = [0]
        
        def generate_embedding(text):
            embedding_counter[0] += 1
            return [0.1 * embedding_counter[0]] * 10
        
        mock_generator.generate.side_effect = generate_embedding
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º
        result = indexer.index_all(mock_generator)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.assertEqual(result.total_files, 2)
        self.assertGreater(result.total_chunks, 2)  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ 2 —á–∞–Ω–∫–æ–≤
        self.assertEqual(len(result.errors), 0)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        loaded = indexer.load_index()
        self.assertIsNotNone(loaded)
        self.assertEqual(len(loaded["chunks"]), result.total_chunks)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —á–∞–Ω–∫–∞
        first_chunk = loaded["chunks"][0]
        self.assertIn("id", first_chunk)
        self.assertIn("text", first_chunk)
        self.assertIn("embedding", first_chunk)
        self.assertIn("source", first_chunk)
        self.assertIn("position", first_chunk)


if __name__ == "__main__":
    print("=" * 60)
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è indexer")
    print("=" * 60)
    
    # –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –≤—ã–≤–æ–¥–æ–º
    unittest.main(verbosity=2)
